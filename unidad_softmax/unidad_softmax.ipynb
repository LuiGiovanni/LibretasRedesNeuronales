{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "# coding: utf-8"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"imagenes/rn3.png\" width=\"200\"><br>\n", "<img src=\"http://www.identidadbuho.uson.mx/assets/letragrama-rgb-150.jpg\" width=\"200\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# [Curso de Redes Neuronales](https://curso-redes-neuronales-unison.github.io/Temario/)<br>\n", "<br>\n", "# Una sola unidad *softmax*<br>\n", "<br>\n", "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/), 18 de febrero de 2019 (\u00faltima revisi\u00f3n).<br>\n", "<br>\n", "En esta libreta vamos a revisar los aspectos b\u00e1sicos del aprendizaje para una unidad *softmax* de $K$ salidas, sin capas ocultas y usando el criterio de p\u00e9rdida de entropia en varias clases. El algoritmo es sencillo pero es importante entenderlo bien antes de pasar a cosas m\u00e1s complicadas.<br>\n", "<br>\n", "Empecemos por inicializar los modulos que vamos a requerir."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["get_ipython().run_line_magic('matplotlib', 'inline')\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from IPython.display import Image  # Esto es para desplegar im\u00e1genes en la libreta"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.rcParams['figure.figsize'] = (20,10)\n", "plt.style.use('ggplot')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. La base de datos a utilizar"]}, {"cell_type": "markdown", "metadata": {}, "source": ["La unidad *softmax* es el \u00faltimo de los tipos b\u00e1sicos de neuronas de salida que revisaremos. Para ejemplificar su uso, vamos a utilizar una base de datos bastante comun, MNIST. MNIST es una base de datos de digitos escritos a mano, en formato de $20 \\times 20$ pixeles. La base completa puede obtenerse en la p\u00e1gina de Yan LeCun (http://yann.lecun.com/exdb/mnist/).<br>\n", "<br>\n", "Nosotros en realidad vamos a utilizar una base de datos reducida de la original y con im\u00e1genes de calidad m\u00e1s reducida ($16 \\times 16$ pixeles por imagen). Numpy prov\u00e9e un m\u00e9todo para guardad objetos tipo numpy en un solo archivo, utilizando el m\u00e9todo de compresi\u00f3n *gunzip*. Los datos ya se encuentran preprocesados y empaquetados en un archivo llamado `digitos.npz`. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = np.load(\"datos/digitos.npz\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Las llaves del diccionario son: \\n{}\".format(data.keys()))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["En este caso, `data` es un objeto contenedor de numpy cuyas llaves son `X_valida`, `X_entrena`, `T_valida`, `T_entrena`. Cada una de estas son a su vez objetos tipo ndarray de numpy, los cuales contienen valores de entrada y salida, tantopara entrenamiento como para validaci\u00f3n. No se preocupen, esto de entrenamiento y validaci\u00f3n lo vamos a ver m\u00e1s adelante en la clase.<br>\n", "<br>\n", "Cada renglon de x es una imagen *desenrrollada*, esto es los 256 datos de una im\u00e1gen de $16 \\times 16$ pixeles. Por otra parte, cada renglon de y es un vector de 10 posiciones, donde todos los valores son ceros, salvo uno, que es el que define la clase de la imagen.<br>\n", "<br>\n", "Para darse una mejor idea, ejecuta el siguiente script varias veces."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = data['X_entrena']\n", "y = data['T_entrena']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["a = np.random.randint(0, y.shape[0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"-- x es de dimensiones {}\".format(x.shape))\n", "print(\"-- y es de dimensiones {}\".format(y.shape))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\ny si escogemos la imagen {} veremos\".format(a))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.imshow(x[a,:].reshape(16,16), cmap=plt.gray())\n", "plt.axis('off')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"la cual es un {}\".format(list(y[a,:]).index(1)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n\\nY si miramos lo que contiene, veremos que\")\n", "print(\"x[a,:] = \\n{}\\ny[a,:] = \\n{}\".format(x[a,:], y[a,:]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["O bien, ejecuta este script varias veces para ver un grupo grande de im\u00e1genes."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["indices = np.arange(y.shape[0])\n", "np.random.shuffle(indices)\n", "ind = indices[0:100].reshape(10,10)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["imagen = np.ones((10 * 16 + 4*11, 10 * 16 + 4*11))\n", "for i in range(10):\n", "    for j in range(10):\n", "        imagen[4 + i * 20: 20 + i * 20, 4 + j * 20: 20 + j * 20] = x[ind[i, j], :].reshape(16,16)\n", "        \n", "plt.imshow(imagen, cmap=plt.gray())\n", "plt.axis('off')\n", "plt.title(u\"Ejemplos aleatorios de im\u00e1genes a clasificar\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Unidad *softmax*"]}, {"cell_type": "markdown", "metadata": {}, "source": ["En una unidad *softmax*, en lugar de estimar solamente un vector de par\u00e1metros, el valor de salida lo descomponemos en $K$ vectores de salida con valores de 1 (si pertenece a esa clase) o cero (si no pertenece). A esto se le conoce como *dummy variable*. En el ejemplo que tenemos, las clases ya se encuentran de esa forma (por eso la salida es un vector de dimensi\u00f3n 10 donde solo uno es 1 y todos los dem\u00e1s valores son 0).  <br>\n", "<br>\n", "El problema de aprendizaje para una unidad *softmax* es estimar una matriz de pesos $W$ y un vector de sesgos tal que:<br>\n", "<br>\n", "$$<br>\n", "W = (w_1, \\ldots, w_K)^T, \\quad b = (b_1, \\ldots, b_K)^T<br>\n", "$$<br>\n", "<br>\n", "donde $w_c = (w_{c,1}, \\ldots, w_{c,n})^T$ para $c = 1, \\ldots, K$, es el vector columna que parametriza la clase $c$. <br>\n", "La probabilidad que el objeto $x^{(i)}$ pertenezca a la clase $c$ est\u00e1 dado por una distribuci\u00f3n *softmax* de la agregaci\u00f3n lineal de cada atributo, esto es:<br>\n", "<br>\n", "$$<br>\n", "z_c^{(i)} = w_c^T x^{(i)} + b_c, \\quad z^{(i)} = (z_1^{(i)}, \\ldots, z_K^{(i)})^T,<br>\n", "$$<br>\n", "<br>\n", "$$<br>\n", "a_c^{(i)} = \\Pr[y^{(i)} = c\\ |\\ x^{(i)}; W, b] = softmax_c(z^{(i)}) = \\frac{\\exp(z_c^{(i)})}{\\sum_{r=1}^K \\exp(z_r^{(i)})}.<br>\n", "$$<br>\n", "<br>\n", "Recuerda que para calcular el valor de *softmax* hay que estar muy pendiente de evitar problemas de estabilidad num\u00e9rica, as\u00ed que cuando lo programes toma en cuenta las recomedaciones que se realizaron en el curso.<br>\n", "<br>\n", "**Implementa el calculo de softmax en forma matricial (para todas las clases de un conjunto de M ejemplos)**<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def softmax(z):\n", "    \"\"\"\n", "    Calculo de la regresi\u00f3n softmax\n", "    \n", "    @param z: ndarray de dimensi\u00f3n (K, M) donde z[:, i] es el vector $z^{(i)}$\n", "    \n", "    @return: un ndarray de dimensi\u00f3n (K, M) donde cada columna es $a^{(i)}$.\n", "    \n", "    \"\"\"\n", "    #--------------------------------------------------------------------------------\n", "    # AGREGA AQUI TU C\u00d3DIGO\n", "    #--------------------------------------------------------------------------------\n", "    z = z.T\n", "    def softaux(x):\n", "        return np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))\n", "    aux = np.array([softaux(x) for x in z])\n", "    return aux.T    \n", "    \n", "    #--------------------------------------------------------------------------------"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Y otra vez con el testunit del pobre (los pueden hacer a mano para verificar)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["z = np.array([[    1,    -10,     -10],\n", "              [    0,      0,       0],\n", "              [-1000,  -1000,   -1000],\n", "              [   10,    -20,     -21],\n", "              [    1,    0.5,    0.9]]).T\n", "a = softmax(z)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(a)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["assert np.all(np.abs(np.sum(a, axis=0) - 1) < 1e-8)\n", "assert a[0, 0] > 0.999\n", "assert a[1, 0] == a[2, 0]\n", "assert 0.33 < a[0, 1] == a[1, 1] == a[2, 1] < 0.34\n", "assert 0.33 < a[0, 2] == a[1, 2] == a[2, 2] < 0.34\n", "assert a[1, 3] > a[2, 3]\n", "assert a[1, 4] < a[2, 4] < a[0, 4]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Y ahora es necesario implementar la funci\u00f3n de p\u00e9rdida, la cual es la suma del logaritmo de la probabilidad estimada para la clase que deber\u00eda haber sido seleccionada (criterio de m\u00ednima entrop\u00eda) tal como lo vimos en clase:<br>\n", "<br>\n", "$$<br>\n", "Loss(W, b) = -\\frac{1}{M}\\sum_{i=1}^M \\sum_{c=1}^K y_c^{(i)} \\log(a_c^{(i)}),<br>\n", "$$<br>\n", "<br>\n", "donde $y_c^{(i)}$ es un valor de 0 o 1 dependiendo si el objeto $i$ pertenece a la clase $c$ o no, mientras que $a_c^{(i)}$ es la probabilidad que el objeto $i$ pertenezca a la clase $k$ conociendo $x^{(i)}$ y parametrizado por W y b. <br>\n", "<br>\n", "**Implementa la funci\u00f3n de p\u00e9rdida de manera relativamente eficiente, utilizando las facilidades que presenta numpy (recuerda usar los trucos vistos para el c\u00e1lculo del logar\u00edtmo de la funci\u00f3n softmax).**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def Loss(x, y, W, b):\n", "    \"\"\"\n", "    Calcula el costo para la regresi\u00f3n softmax parametrizada por theta, \n", "    con el conjunto de datos dado por (x, y)\n", "    \n", "    @param x: ndarray de dimensi\u00f3n (M, n) con los datos\n", "    @param y: ndarray de dimensi\u00f3n (M, K) con la clase por cada dato\n", "    @param W: ndarray de dimensi\u00f3n (K, n) con los pesos\n", "    @param b: ndarray de dimensi\u00f3n (K,) con los sesgos\n", "    \n", "    @return: Un valor flotante con la p\u00e9rdida utilizando m\u00ednima entrop\u00eda\n", "    \n", "    \"\"\"\n", "    M, K = y.shape\n", "    n = x.shape[1]\n", "    \n", "    #--------------------------------------------------------------------------------\n", "    # AGREGA AQUI TU C\u00d3DIGO\n", "    #--------------------------------------------------------------------------------\n", "    z = x @ W.T + b\n", "    a = softmax(z.T)\n", "    return (-1/M) * np.sum(y*np.log(a))    \n", "    \n", "    #--------------------------------------------------------------------------------"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def test_perdida():\n", "    x = np.array([[0, 0], \n", "                  [0, 1], \n", "                  [1, 0], \n", "                  [1, 1]])\n", "    y = np.eye(4)\n", "    W = np.array([[-4,  -4],\n", "                  [-1,   3],\n", "                  [ 3, -10],\n", "                  [ 5,   5]])\n", "                \n", "    b = np.array([3, -1, 0.01, -5])\n", "    \n", "    #print(Loss(x, y, W, b))\n", "    \n", "    assert 0.08 < Loss(x, y, W, b) < 0.09\n", "    return \"Paso la prueba\"\n", "    \n", "print(test_perdida())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Implementa la funci\u00f3n para predecir el valor de $y$ estimada, basandose en el principio de *maximum a posteriori.**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def predice(x, W, b):\n", "    \"\"\"\n", "    Prediccion de y_hat a partir de la matriz de pesos W y el vector de sesgos b \n", "    para los valores de x\n", "    \n", "    @param x: ndarray de dimensi\u00f3n (M, n) con los datos\n", "    @param W: ndarray de dimensi\u00f3n (K, n) con los pesos\n", "    @param b: ndarray de dimensi\u00f3n (K, ) con los sesgos\n", "    @return: ndarray de dimensi\u00f3n (M, K) con la clase predecida \n", "             por cada dato en formato dummy (unos y ceros)\n", "    \n", "    \"\"\"\n", "    #--------------------------------------------------------------------------------\n", "    # AGREGA AQUI TU C\u00d3DIGO\n", "    #--------------------------------------------------------------------------------\n", "    z = x @ W.T + b\n", "    a = softmax(z.T)\n", "    return a.T > 0.5\n", "    \n", "    #--------------------------------------------------------------------------------"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def prueba_prediccion():\n", "    x = np.array([[0, 0], \n", "                  [0, 1], \n", "                  [1, 0], \n", "                  [1, 1],\n", "                  [0, 0], \n", "                  [0, 1], \n", "                  [1, 0], \n", "                  [1, 1]])\n", "    \n", "    y = np.r_[np.eye(4), np.eye(4)]\n", "    \n", "    W = np.array([[-4,  -4],\n", "                  [-1,   3],\n", "                  [ 3, -10],\n", "                  [ 5,   5]])\n", "    b = np.array([3, -1, 0.01, -5])\n", "        \n", "    assert abs((y - predice(x, W, b)).sum()) < 1e-12 \n", "    print(\"Paso la prueba\")\n", "    \n", "prueba_prediccion()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Y por \u00faltimo tenemos que implementar el gradiente para poder utilizar los m\u00e9todos de optimizaci\u00f3n (ya sea por descenso de gradiente o por alg\u00fan m\u00e9todo de optimizaci\u00f3n.<br>\n", "<br>\n", "El gradiente se obtiene a partir de las derivadas parciales:<br>\n", "<br>\n", "$$<br>\n", "\\frac{\\partial Loss(W, b)}{\\partial w_{c,j}} = - \\frac{1}{M} \\sum_{i = 1}^M \\left(y_c^{(i)} - a_c^{i}\\right) x_j^{(i)}, \\qquad \\frac{\\partial Loss(W, b)}{\\partial b_c} = - \\frac{1}{M} \\sum_{i = 1}^M \\left(y_c^{(i)} - a_c^{i}\\right)<br>\n", "$$<br>\n", "<br>\n", "**Implementa una funci\u00f3n para el c\u00e1lculo del gradiente.**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def gradiente(x, y, W, b):\n", "    \"\"\"\n", "    Calculo del gradiente para el problema de regresi\u00f3n softmax\n", "    \n", "    @param x: ndarray de dimensi\u00f3n (M, n) con los datos\n", "    @param y: ndarray de dimensi\u00f3n (M, K) con la clase (dummy) por cada dato\n", "    @param W: ndarray de dimensi\u00f3n (K, n) con los pesos\n", "    @param b: ndarray de dmensi\u00f3n (K, ) con los sesgos\n", "    \n", "    @return: dW, db con los gradientes de Loss respecto a W y b respectivamente\n", "    \n", "    \"\"\"\n", "    #--------------------------------------------------------------------------------\n", "    # AGREGA AQUI TU C\u00d3DIGO\n", "    #--------------------------------------------------------------------------------\n", "    M = x.shape[0]\n", "    z = x @ W.T + b\n", "    a = softmax(z.T)\n", "    dW = (-1/M) * (y-a) @ x\n", "    db = (-1/M) * np.sum(y-a, axis = 1)\n", "    \n", "    #--------------------------------------------------------------------------------\n", "    return dW, db"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def prueba_gradiente():\n", "    \n", "    x = np.array([[0, 0], \n", "                  [0, 1], \n", "                  [1, 0], \n", "                  [1, 1]])\n", "    \n", "    y = np.eye(4)\n", "    W = np.array([[-4,  -4],\n", "                  [-1,   3],\n", "                  [ 3, -10],\n", "                  [ 5,   5]])\n", "    b = np.array([3, -1, 0.01, -5])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    dW, db = gradiente(x, y, W, b)\n", "    \n", "    assert dW.shape == W.shape\n", "    assert db.shape == db.shape\n", "    assert np.all(np.abs(db - np.array([-0.001, -0.028, -0.005, 0.035])) < 0.001)\n", "    assert -0.0304 < dW.trace() < -0.0302\n", "    print(\"Paso la prueba\")\n", "    \n", "prueba_gradiente() \n", "    \n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora si, ya nos encontramos en posibilidad para realizar el aprendizaje en una unidad softmax.<br>\n", "<br>\n", "**Desarrolla el aprendizaje por descenso de gradiente (similaral de las otras libretas)**."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def dg_softmax(x, y, W, b, alpha, max_iter=10000, tol=1e-3, historial=False):\n", "    \"\"\"\n", "    Descenso de gradiente por lotes para la clasificaci\u00f3n softmax\n", "    \n", "    ---AGREGA AQUI LA DOCUMENTACI\u00d3N---\n", "    \n", "    \"\"\"\n", "    if historial:\n", "        historial_loss = np.zeros(max_iter)\n", "        historial_loss[0] = Loss(x, y, W, b)\n", "    else:\n", "        historial_loss = None\n", "        \n", "    for iter in range(1, max_iter):\n", "        #--------------------------------------------------------------------------------\n", "        # AGREGA AQUI TU C\u00d3DIGO\n", "        #--------------------------------------------------------------------------------\n", "        dw,db = gradiente(x,y.T,W,b)\n", "        W+= -dw*alpha\n", "        b+= -db*alpha\n", "        J = Loss ( x, y.T, W, b)\n", "        if J < tol:\n", "            return W, b, historial_loss\n", "        if historial:\n", "            historial_loss[iter] = J  \n\n", "        #--------------------------------------------------------------------------------\n", "    return W, b, historial_loss\n", "        \n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Pero para utilizar el descenso de gradiente hay que ajustar un valor de `alpha`."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ajusta un valor de epsilon razonable"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["alpha_prueba = 0.06#---usa esto para buscar un valor aceptable de alpha---"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["M, K = y.shape\n", "n = x.shape[1]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["W = np.zeros((K, n))\n", "b = np.zeros(K)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["_, _, loss_hist = dg_softmax(x, y, W, b, alpha_prueba, max_iter=800, historial=True)\n", "plt.plot(range(50), loss_hist)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["y para probarlo vamos a aprender a clasificar a los digitos de nuestra base de datos<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["W = np.zeros((n, K))\n", "b = np.zeros(K)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["alpha = alpha_prueba\n", "W, b, _ = dg_softmax(x, y, W, b, alpha, max_iter=2000)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"El costo de la soluci\u00f3n final es de {}\".format(Loss(x, y, W, b)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_estimada = predice(x, W, b)\n", "errores = np.where(y.argmax(axis=1) == y_estimada.argmax(axis=1), 0, 1)\n", "print(\"\\nLos datos utilizados para el aprendizaje y mal clasificados son el {}%,\".format(100 * errores.mean()))\n", "print(\"esto es, de {} datos, se clasificaron incorrectamente {}\".format(x.shape[0], errores.sum()))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Esto solo es para hacerla m\u00e1s emocionante"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_test = data['X_valida']\n", "y_test = data['T_valida']\n", "y_estimada_T = predice(x_test, W, b)\n", "errores = np.where(y_test.argmax(axis=1) == y_estimada_T.argmax(axis=1), 0, 1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\nY con los datos de pureba el error es del {}%,\".format(100 * errores.mean()))\n", "print(\"esto es, de {} datos, se clasificaron incorrectamente {}\".format(x_test.shape[0], errores.sum()))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u00bfSer\u00e1 esta la mejor soluci\u00f3n? \u00bfSer\u00e1 una buena soluci\u00f3n? Por esto no hay que preocuparse mucho todav\u00eda, lo vamos a revisar m\u00e1s adelante en el curso. Se espera con la unidad *softmax* poder clasificar correctamente m\u00e1s del 97% de los datos de entrenamiento y m\u00e1s del 94% de los datos de validaci\u00f3n. "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}