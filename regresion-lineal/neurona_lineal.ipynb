{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "# coding: utf-8"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"imagenes/rn3.png\" width=\"200\"><br>\n", "<img src=\"http://www.identidadbuho.uson.mx/assets/letragrama-rgb-150.jpg\" width=\"200\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# [Curso de Redes Neuronales](https://curso-redes-neuronales-unison.github.io/Temario/)<br>\n", "<br>\n", "# Una sola neurona lineal<br>\n", "<br>\n", "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/) <br>\n", "<br>\n", "*Ultima revisi\u00f3n*: 31 de enero de 2019.<br>\n", "<br>\n", "En esta libreta vamos a revisar los aspectos b\u00e1sicos del aprendizaje para una sola neurona de salida lineal, sin capas ocultas y usando el criterio de p\u00e9rdida MSE. El algoritmo es muy simple pero es importante tener claro lo que se requiere antes de pasar a cosas m\u00e1s complicadas.<br>\n", "<br>\n", "Empecemos por inicializar los modulos que vamos a requerir."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["get_ipython().run_line_magic('matplotlib', 'inline')\n", "import numpy as np\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.rcParams['figure.figsize'] = (16,8)\n", "plt.style.use('ggplot')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 1. Un ejemplo en una sola dimensi\u00f3n<br>\n", "<br>\n", "Una funci\u00f3n muy importante para poder realizar aprendizaje m\u00e1quina es la capacidad de poder manejar, cargar y guardar datos. en esta libreta vamos a empezar con lo m\u00e1s b\u00e1sico: leer datos desde un archivo texto (o un archivo.cvs). M\u00e1s adelante revisaremos como recolectar datos de internet, de archivos tipo excel o de bases de datos.<br>\n", "<br>\n", "*Numpy* cuenta con varios m\u00e9todos para leer y guardar datos. La m\u00e1s utilizada para cargar datos provenientes de un archivo de texto es `loadtxt`. Para obtener la documentaci\u00f3n de la funci\u00f3n, simplemente ejecuta la celda siguiente: "]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["get_ipython().run_line_magic('pinfo', 'np.loadtxt')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Es importante ver que esta funci\u00f3n directamente carga los datos existentes en el archivo en un `ndarray`. \u00bfY si tenemos uno o varios `ndarrays` con las cosas que hemos desarrollado y los queremos guardar en disco (por ejemplo el vector $\\omega$ de par\u00e1metros)? <br>\n", "<br>\n", "Vamos a abrir y a visualizar unos datos que se encuentran en el archivo `carretas.txt` (abrelos con un editor de texto si quieres ver el archivo original). En este archivo se tiene las ganancias anuales (en dolares) de unos tacos de carreta (bueno, su equivalente gringo) respecto al tama\u00f1o de la ciudad donde se encuentra la carreta. Estos datos provienen de el curso de *Machine learning* de *coursera* de *Andrew Ng*.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lee los datos en un nd array llamado datos"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["datos = np.loadtxt('datos/carretas.txt', comments='%', delimiter=',')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Separa los datos de entrada de los de salida.<br>\n", "si decimos que x = datos[:,0], pues se toma solo una columna de datos,<br>\n", "por lo que x ser\u00eda un ndarray de forma (shape) (96,). Al decir x = datos[:, 0:1] <br>\n", "significa que vamos a tomar todas las columnas de 0 a una antes de 1, por lo<br>\n", "que x tiene una forma (96, 1). Para mantener generalidad, es mejor manejar x xomo una matriz<br>\n", "de una sola columna que como un vector de una dimensi\u00f3n."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x, y = datos[:,0:1], datos[:,1]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["T es el n\u00famero de instancias y n el de atributos"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["T, n = x.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(x, y, 'rx')\n", "plt.title(u'Ganancias anuales de una carreta de acuerdo al tama\u00f1o de una ciudad')\n", "plt.xlabel(r\"Poblaci$\\'o$n ($\\times 10^4$ habitantes)\")\n", "plt.ylabel(r'Beneficios ($\\times 10^4$ dolares)')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Listo, ya temos los datos. La hip\u00f3tesis que hacemos es que el valor de salida lo podemos estimar como<br>\n", "<br>\n", "$$<br>\n", "\\hat{y}^{(i)} = h_\\omega(x^{(i)}) = \\omega_1 x^{(i)} + b<br>\n", "$$<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["El criterio MSE como funci\u00f3n de p\u00e9rdida es el de minimizar el costo definido como<br>\n", "$$<br>\n", "Loss(\\omega, b) = \\frac{1}{2M} \\sum_{i = 1}^M (y^{(i)} - \\hat{y}^{(i)})^2.<br>\n", "$$<br>\n", "<br>\n", "Por lo tanto, para saber si estamos minimizando o no, debemos ser capaces de medir la funci\u00f3n de p\u00e9rdida. <br>\n", "<br>\n", "**Desarrolla la funci\u00f3n de p\u00e9rdida tal como se pide abajo**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def mse_loss(x, y, w, b):\n", "    \"\"\"\n", "    Calcula el costo de acuerdo al criterio de MSE (mean square error) asumiendo un conjunto de datos\n", "    x, con una salida y, y una hip\u00f3tesis lineal parametrizada por omega\n", "    \n", "    Par\u00e1metros\n", "    ----------\n", "    x: Un ndarray de dimension (M, n)\n", "    y: Un ndarray de dimensi\u00f3n (M, )\n", "    w: Un ndarray de dimensi\u00f3n (n, )\n", "    b: Un flotante\n", "    \n", "    Devuelve\n", "    --------\n", "    Un flotante con el costo\n", "    \n", "    \"\"\"\n", "    M, n = x.shape\n", "        \n", "    # Puedes hacerlo en forma de ciclos\n", "    # J = 0\n", "    # for instancia in range(M):\n", "    #    J += --inserta aqui tu c\u00f3digo--\n", "    # return --inserta aqu\u00ed tu c\u00f3digo--\n", "    \n", "    # Puedes hacerlo directamente en forma matricial (as\u00ed deber\u00eda de ser)\n", "    error = y - (x @ w + b)\n", "    return np.mean(np.square(error))/2\n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["y para probar si est\u00e1 bien el programa, si calculamos $Loss(\\omega, b)$ para $\\omega_1 = 0$, $b = 1$ debe de dar (para este conjunto de datos) **26.73** (recuerda verificar que regrese un flotante y no un vector de un solo elemento)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w = np.zeros([n])\n", "b = 1\n", "print(\"La p\u00e9rdida es {}\".format(mse_loss(x, y, w, b)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Muy bien, ya podemos calcular el criterio a optimizar (la funci\u00f3n de p\u00e9rdida). Vamos entonces a utilizar la funci\u00f3n que acabamos de hacer para ver sus valores para diferentes valores de $\\omega$ y $b$."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Definimos una funci\u00f3n que depende solo de b y theta1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def costo_w(b, w1):\n", "    return mse_loss(x, y, np.array([w1]), b)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Y ahora la convertimos en una funci\u00f3n tipo numpy (aplica para cualquier entrada de ndarrays)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["costo_vect = np.frompyfunc(costo_w, 2, 1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["hora generamos la lista de valores para graficar"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["b = np.linspace(-15, 10, 100);\n", "w1 = np.linspace(-2, 4, 100);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Y los convertimos en matrices utilizando la funci\u00f3n meshgrid"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["b, w1 = np.meshgrid(b, w1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Y calculamos los costos para cada par de theta0 y theta 1 con nuestra nueva funcion de costos vectorizada"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["J = costo_vect(b, w1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Y graficamos el contorno"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.contour(b, w1, J, 80, linewidths=0.5, colors='k')\n", "plt.contourf(b, w1, J, 80, cmap=plt.cm.rainbow, vmax=J.max(), vmin=J.min())\n", "plt.colorbar()\n", "plt.xlabel(r\"$b$\")\n", "plt.ylabel(r\"$\\omega_1$\")\n", "plt.title(r\"Funcion de perdida\")\n", "        "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora si, ya tenemos todo para hacer nuestra funci\u00f3n para encontrar los par\u00e1metros que optimicen la funci\u00f3n de costo (que como se puede ver en la superficie deber\u00eda de estar por donde $b$ vale entre 0 y -5 y $\\omega_1$ entre 1 y 2). <br>\n", "<br>\n", "**Desarrolla la funci\u00f3n con descenso de gradiente.**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def grad_costo(x, y, w, b):\n", "    \"\"\"\n", "    Calcula el gradiente respecto a w y b de los datos existentes\n", "    \n", "    Par\u00e1metros\n", "    ----------\n", "    x: ndarray de dimension [M, n] con los datos de entrada\n", "    y: ndarray de dimension [M,] con los datos de salida\n", "    w: ndarray de dimension [n, ] con los pesos \n", "    b: flotante con el sesgo \n", "    \n", "    Devuelve\n", "    --------\n", "    dw, db: donde dw es un vector de dimension de w con el gradiente\n", "            de la funci\u00f3n de costo respecto a w, y db es la derivada de la\n", "            funcion de costo respecto a b\n", "    \"\"\"\n", "    error = y - (x @ w + b)\n\n", "    # --aqui hay que poner c\u00f3digo--        \n", "    dw = -x.T @ error / y.shape[0]\n", "    db = -error.mean()\n", "    \n", "    #------------------------------\n", "    return dw, db "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def descenso_gradiente_lotes(x, y, w_0, b_0, alpha, num_iter):\n", "    \"\"\"\n", "    Descenso de gradiente durante num_iter iteraciones para regresi\u00f3n lineal\n", "    \n", "    Par\u00e1metros\n", "    -----------\n", "    x: ndarray de dimension [M, n] con los datos de entrada\n", "    y: ndarray de dimension [M,] con los datos de salida\n", "    w_0: ndarray de dimension [n, ] con los pesos iniciales\n", "    b_0: flotante con el sesgo inicial\n", "    alpha: flotante con tama\u00f1o de paso o tasa de aprendizaje.\n", "    num_iter: numero de iteraciones (entero)\n", "    \n", "    Devuelve\n", "    --------\n", "    w, b, mse_iter: donde w y b tiene las dimensiones de w_0 y b_0 con los par\u00e1metros \n", "                    aprendidos, mientras que mse_hist es un ndarray de dimensi\u00f3n \n", "                    [num_iter, 1] con el costo en cada iteraci\u00f3n.\n", "    \n", "    \"\"\"\n", "    w, b = w_0.copy(), b_0\n", "    mse_iter = np.zeros(num_iter)    \n", "    M, n = x.shape\n", "    \n", "    for iter in range(num_iter):\n", "        \n", "        dw, db = grad_costo(x, y, w, b)\n", "        \n", "        # --aqui hay que poner c\u00f3digo--        \n", "        w += -dw*alpha\n", "        b += -db*alpha\n", "        \n", "        #------------------------------\n", "        \n", "        mse_iter[iter] = mse_loss(x, y, w, b)\n", "    return w, b, mse_iter\n", "     "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Y para saber si el algoritmo se programo bien, se puede probar en el problema del *food truck* y revisar si el valor de la funci\u00f3n de p\u00e9rdida se va reduciendo hasta estabilizarse en un m\u00ednimo."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w_0 = np.zeros((n,))\n", "b_0 = 0.0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["iteraciones = 1500\n", "alpha = 0.01"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w, b, mse_historial = descenso_gradiente_lotes(x, y, w_0, b_0, alpha, iteraciones)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(mse_historial, 'b')\n", "plt.title(u'MSE por iteraci\u00f3n')\n", "plt.xlabel(u'iteraci\u00f3n')\n", "plt.ylabel(r'$Loss(\\omega, b)$')\n", "plt.figtext(x=.6, y=.6, s=\"Al final de las iteraciones:\\n\\n w1 = {}\\n b    = {}\".format(w[0], b))\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora vamos a revisar virtualmente si la estimaci\u00f3n es una linea recta que pasa entre todos los puntos.<br>\n", "<br>\n", "**Completa los pasos para realizar la estimaci\u00f3n.**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_estimar = np.array([[4],[24]])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<br>\n", "Agrega el codigo necesario<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_estimado = x_estimar @ w + b"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Los valores estimados son: {}\".format(y_estimado))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Si los valores que obtuviste son cercanos a 1 (10000 dolares) y 24.3 (243000 dolares) entonces estamos en los valores esperados. Ahora vamos a usar estos valores para graficar los datos reales y la estimaci\u00f3n realizada:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(x[:], y, 'xr')\n", "plt.plot(x_estimar[:,0], y_estimado, '-b')\n", "plt.title(u'Ganancias anuales de una carreta de acuerso al tama\u00f1o de una ciudad')\n", "plt.xlabel(r\"Poblaci$\\'o$n ($\\times 10^4$ habitantes)\")\n", "plt.ylabel(r'Beneficios ($\\times 10^4$ dolares)')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**\u00a1Felicidades!** Acabas de terminar el algoritmo de aprendizaje m\u00e1s usado en el mundo."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 2. Un ejemplo en multiples dimensiones<br>\n", "<br>\n", "Como el algortimo realizado ya funciona para muchas dimensiones, no se espera tener mucho problema para utilizarlos. As\u00ed que ahora vamos a cargar datos y vamos a graficar la salida respecto a dos variables"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["datos = np.loadtxt('datos/casas_portland.txt', comments='%', delimiter=',')\n", "x, y = datos[:, :-1], datos[:,-1] "]}, {"cell_type": "markdown", "metadata": {}, "source": ["M es el n\u00famero de instancias y n el de atributos"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["M, n = x.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(x[:,0], y, 'rx')\n", "plt.title(u'Costo de una casa en relaci\u00f3n a su tama\u00f1o')\n", "plt.xlabel(u\"tama\u00f1o (pies cuadrados)\")\n", "plt.ylabel('costo ')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure()\n", "plt.plot(x[:,1], y, 'rx')\n", "plt.title(u'Costo de una casa en relaci\u00f3n al n\u00famero de cuartos')\n", "plt.xlabel(\"cuartos\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Antes de realizar el aprendizaje podemos ver que mientras una de las variables se mide en miles de pies cuadrados, la otra variable tiene valores de 1 a 4. Esto es un problema para el algoritmo del descenso de gradiente, por lo que es necesario normalizar los datos (solo para este algoritmo) y que funcione de manera correcta. <br>\n", "<br>\n", "Para normalizar requerimos de dos pasos, por un lado, obtener los valores de medias y desviaciones estandares por atributo, y en segundo lugar, realizar la normalizaci\u00f3n. Los valores de medias y desviaciones estandares hay que guardarlos, ya que ser\u00e1n necesarios para poder normalizar los datos que se quiera estimar.<br>\n", "<br>\n", "**Escribe la funci\u00f3n que devuelve los valores de medias t desviaciones estandares.**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def obtiene_medias_desviaciones(x):\n", "    \"\"\"\n", "    Obtiene las medias y las desviaciones estandar atributo a atributo.\n", "    \n", "    Par\u00e1metros\n", "    ----------\n", "    x: un ndarray de dimensi\u00f3n (T, n) donde T es el n\u00famro de elementos y \n", "       n el n\u00famero de atributos\n", "    \n", "    Devuelve\n", "    ---------\n", "    medias, desviaciones: donde ambos son ndarrays de dimensiones (n,) con \n", "                          las medias y las desviaciones estandar respectivamente.\n", "    \n", "    \"\"\"\n", "    # Escribe aqui el c\u00f3digo\n", "    return np.mean(x,axis=0),np.std(x,axis=0)\n", "    #    \n", "    \n", "def normaliza(x, medias, desviaciones):\n", "    \"\"\"\n", "    Normaliza los datos x\n", "    Par\u00e1metros\n", "    ----------\n", "    x: un ndarray de dimensi\u00f3n (T, n) donde T es el n\u00famro de elementos y n el n\u00famero de atributos\n", "    medias: ndarray de dimensiones (n,) con las medias con las que se normalizar\u00e1\n", "    desviaciones: ndarray de dimensiones (n,) con las desviaciones con las que se normalizar\u00e1\n", "    \n", "    Devuelve\n", "    --------\n", "    x_norm, un ndarray de las mismas dimensiones de x pero normalizado\n", "    \n", "    \"\"\"\n", "    return (x - medias) / desviaciones\n", "        "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Y ahora vamos a hacer algo muy simple para probar, que pueden corroborar con el uso de una calculadora com\u00fan."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_prueba = np.array([[1, 300],\n", "                    [3, 100],\n", "                    [2, 400],\n", "                    [4, 200]])\n", "m, d = obtiene_medias_desviaciones(x_prueba)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Los datos son: \\n{}\".format(x_prueba))\n", "print(\"Las medias son: \\n{}\".format(m))\n", "print(\"Las desviaciones son: \\n{}\".format(d))\n", "print(\"Los datos normalizados son: \\n{}\".format(normaliza(x_prueba, m, d)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Listo, entonces ya podemos hacer descenso de gradiente, o casi. El problema es que no sabemos cual ser\u00eda el mejor valor para $\\alpha$. Escoge el valor de $\\alpha$ realizando una gr\u00e1fica de 50 iteraciones solamente para valores desde 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, ... y decide cual de los valores es el que m\u00e1s te conviene.<br>\n", "<br>\n", "**Selecciona un valor, especifica aqu\u00ed cual es, y justifica porque lo seleccionaste.**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["medias, desviaciones = obtiene_medias_desviaciones(x)\n", "x_norm = normaliza(x, medias, desviaciones)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w_ini = np.zeros((n,))\n", "b_ini = 0\n", "num_iters = 50"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Aqui es donde hay que hacer las pruebas"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["alpha = 0.1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["_, _, mse_hist = descenso_gradiente_lotes(x_norm, y, w_ini, b_ini, alpha, num_iters)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(mse_hist, '-b')\n", "plt.title(r\"La curva de aprendizaje para $\\alpha =$ \" + str(alpha))\n", "plt.xlabel('iteraciones')\n", "plt.ylabel('MSE')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Utilizando las iteraciones necesarias, encuentra el valor de $\\omega$ y $b$ utilizando el descenso de gradiente.**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Aqui ya no pongo c\u00f3digo, esto debe ser relativamente simple"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w_ini = np.zeros((n,))\n", "b_ini = 0\n", "num_iters = 120"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w, b, mse_hist = descenso_gradiente_lotes(x_norm, y, w_ini, b_ini, alpha, num_iters)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(mse_hist, '-b')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.xlabel('iteraciones')\n", "plt.ylabel('MSE')\n", "# **Obten el valor de una casa de 1650 pies cuadrados y 3 recamaras con el modelo obtenido (recuerda que hay que normalizar).**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Escribe aqu\u00ed el c\u00f3digo"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["casa_piloto = np.array([[1650,3]])\n", "casa_piloto = normaliza(casa_piloto,medias,desviaciones)\n", "print(\"Valor estimado de una casa de 1650 pies cuadrados y 3 recamaras:\")\n", "valor_estimado = casa_piloto @ w  + b\n", "print(valor_estimado[0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3. Regularizaci\u00f3n<br>\n", "<br>\n", "Un detalle que se nos falta revisar es el efecto de a regularizaci\u00f3n en la regresi\u00f3n, y lo vamos a realizar a partir de un ejemplo sobre ajuste de curvas con polinomios, generando datos sint\u00e9ticos. La funci\u00f3n que vamos a tratar de ajustar, utilizando 20 puntos seleccionados al azar es:<br>\n", "<br>\n", "$$f(x) = \\exp(-\\frac{x}{2}) \\sin(2x)$$<br>\n", "<br>\n", "Para esto, vamos a generar (utilizando una semilla com\u00fan, para que todos obtengamos los mismos resultados) datos provenientes de un modelo que es un polinomio de orden superior.<br>\n", "<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rng = np.random.RandomState(10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Funci\u00f3n a aproximar"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def f(x): return np.exp(-x/2) * np.sin(2*x)\n", " \n", "# Datos para graficar\n", "x_plot = np.linspace(0, 10, 1000)\n", "y_plot = f(x_plot)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Genera puntos y selecciona 20 en forma aleatoria "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_train = np.linspace(0, 10, 100)\n", "rng.shuffle(x_train)\n", "x_train = np.sort(x_train[:25])\n", "y_train = f(x_train) + 0.1 * np.random.randn(25)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Graficamos la funci\u00f3n original y los puntos que vamos a utilizar para el aprendizaje"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(x_plot, y_plot, label=\"La funci\u00f3n original\")\n", "plt.scatter(x_train, y_train, label=\"datos para el entrenamiento\")\n", "plt.title(\"Datos sint\u00e9ticos para revisar el uso de regularizaci\u00f3n\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora vamos a ver que pasa si tratamos de ajustar un modelo con diferentes polinomios. La generaci\u00f3n de atributos polinomiales la incluyo en este caso, ya que lo que nos interesa es estudiar el efecto de la regularizacion sobre todo.<br>\n", "<br>\n", "Para ver el problema de utilizar el m\u00e9todo de desceno de gradiente utilizando varias variables vamos a ver el caso sin regularizar primero. Como ejemplo, vamos a poner el resultado obtenido con el m\u00e9todo ana\u00edtico. Como vemos (o veremos) en la figura, el uso de un polinomio de orden 20 (mayor) no ofrece forzosamente una soluci\u00f3n que se apegue a la funci\u00f3n original."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(x_plot, y_plot, label=\"La funci\u00f3n original\")\n", "plt.scatter(x_train, y_train, label=\"datos para el entrenamiento\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for g in [3, 5, 8, 15]:\n\n", "    # Expansi\u00f3n a un polinomio de orden ng\n", "    x_g = np.power(x_train.reshape(-1,1), range(0, g + 1))\n", "    x_g_plot = np.power(x_plot.reshape(-1,1), range(0, g + 1))\n", "    \n", "    w = np.linalg.pinv(x_g) @ y_train    \n", "    y_g = x_g_plot @ w \n", "    \n", "    plt.plot(x_plot, y_g, label=\"polinomio orden {}\".format(g))\n", "    \n", "plt.legend()\n", "plt.axis([0, 10, -0.5, 1])\n", "   \n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["<br>\n", "El m\u00e9todo de descenso de gradiente est\u00e1 lejos de ser un m\u00e9todo \u00f3ptimo y/o eficiente, cuando se tiene soluciones anal\u00edticas, o cuando es posible obtener en forma anal\u00edtica la segunda derivada (y por lo tanto metodos como gradiente conjugado o el de Newton se pueden realizar). Con el fin de ilustrar esto, realiza los siguientes ejercicios.<br>\n", "<br>\n", "**Desarrolla una funci\u00f3n que utilice el m\u00e9todo de descenso de gradiente y obtenga <br>\n", "resultados similares a los de la ecuaci\u00f3n normal.**<br>\n", "<br>\n", "**\u00bfQue pasa si no utlizas la normalizaci\u00f3n de datos?**<br>\n", "<br>\n", "**\u00bfCu\u00e1l es la mejor $\\alpha$ para el ajuste del polinomio de orden 8?**<br>\n", "<br>\n", "**\u00bfCu\u00e1l es el n\u00famero ideal de *epochs*?**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(x_plot, y_plot, label=\"La funci\u00f3n original\")\n", "plt.scatter(x_train, y_train, label=\"datos para el entrenamiento\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for g in [3, 5, 8, 18]:\n", "    w_ini = np.random.randn(g)\n", "    # Expansi\u00f3n a un polinomio de orden ng\n", "    x_g = np.power(x_train.reshape(-1,1), range(1, g + 1))\n", "    x_g_plot = np.power(x_plot.reshape(-1,1), range(1, g + 1))\n\n", "    # Escribe aqui el c\u00f3digo\n", "    #\n", "    #  1. Normaliza (ambos conjuntos)\n", "    m,d = obtiene_medias_desviaciones(x_g)\n", "    x_g = normaliza(x_g,m,d)\n", "    x_g_plot = normaliza(x_g_plot,m,d)\n", "    #  2. Entrenamiento (con los datos de aprendizaje)\n", "    w,b,mse = descenso_gradiente_lotes(x_g, y_train, w_ini, 0.0, 0.15, 12000)\n", "    #  3. Reconocimiento (con lo datos de graficaci\u00f3n) \n", "    #  El resultado se guarda en y_g\n", "    y_g = x_g_plot @ w + b\n", "    #\n", "    #\n", "    #\n", "    plt.plot(x_plot, y_g, label=\"polinomio orden {}\".format(g))\n", "    \n", "plt.legend()\n", "plt.axis([0, 10, -0.5, 1]) \n", "plt.show()\n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["La regularizaci\u00f3n es un m\u00e9todo para poder manejar una cantidad importante de par\u00e1metrs y que siga funcionando el sistema de alguna manera. Los m\u00e9todos de regularizaci\u00f3n m\u00e1s comunes son<br>\n", "<br>\n", "- Regularizaci\u00f3n L2: $$reg(w) = w^Tw$$<br>\n", "- Regularizaci\u00f3n L1: $$reg(w) = \\sum_{i=1}^n |\\omega_i|$$<br>\n", "<br>\n", "y a la funci\u00f3n objetivo para optimizaci\u00f3n (costo) es en adelante la ponderaci\u00f3n de la funci\u00f3n de p\u00e9rdida por la regularizaci\u00f3n.<br>\n", "<br>\n", "$$costo(w, b) = Loss(w, b) + \\frac{\\lambda}{M} reg(w)$$<br>\n", "<br>\n", "As\u00ed, el costo se usa para optimizar, aunque lo que realmente nos interese es la funci\u00f3n de p\u00e9rdida.<br>\n", "<br>\n", "**Completa el c\u00f3digo para calcular el costo y el gradiente del costo para poder ser utilizados dentro del algoritmo de descenso de gradiente.**<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def costo(x, y, w, b, lmda=0.0, tipo='L2'):\n", "    \"\"\"\n", "    Calcula el costo MSE con regularizaci\u00f3n\n", "    \n", "    Par\u00e1metros\n", "    ----------\n", "    x: Un ndarray de dimension (M, n)\n", "    y: Un ndarray de dimensi\u00f3n (M, )\n", "    w: Un ndarray de dimensi\u00f3n (n, )\n", "    b: Un flotante\n", "    lmda: Un flotante con el valor de lambda para la regularizaci\u00f3n\n", "    tipo: string con el tipo de regularizaci\u00f3n ('L2' o 'L1')\n", "    \n", "    Devuelve\n", "    --------\n", "    Un flotante con el costo    \n", "    \n", "    \"\"\"\n", "    M, n = x.shape\n", "    return mse_loss(x, y, w, b) + (lmda / M) * reg(w, tipo)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def reg(w, tipo):\n", "    \"\"\"\n", "    Calcula la regularizaci\u00f3n, que solo depende de los par\u00e1metros\n", "    Par\u00e1metros\n", "    ----------\n", "    w: Un ndarray de dimensi\u00f3n (n, )\n", "    tipo: string con el tipo de regularizaci\u00f3n ('L2' o 'L1')\n", "    \n", "    Devuelve\n", "    --------\n", "    Un flotante con el valor del costo de regularizaci\u00f3n\n", "    \"\"\"\n", "    if tipo is 'L2':\n", "        return w.T @ w\n", "    elif tipo is 'L1':\n", "        return np.sum(np.abs(w))\n", "    else:\n", "        raise ValueError(\"Solo est\u00e1 programada por el momento la regularizaci\u00f3n para L1 y L2\")        \n", "    return None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def grad_costo_reg(x, y, w, b, lmda, tipo):\n", "    \"\"\"\n", "    Calcula el gradiente respecto a w y b de los datos existentes\n", "    \n", "    Par\u00e1metros\n", "    ----------\n", "    x: ndarray de dimension [M, n] con los datos de entrada\n", "    y: ndarray de dimension [M,] con los datos de salida\n", "    w: ndarray de dimension [n, ] con los pesos \n", "    b: flotante con el sesgo \n", "    lmda: Un flotante con el valor de lambda para la regularizaci\u00f3n\n", "    tipo: string con el tipo de regularizaci\u00f3n ('L2' o 'L1')\n", "    \n", "    Devuelve\n", "    ---------\n", "    dw, db donde dw es un vector de dimension de w con el gradiente\n", "           de la funci\u00f3n de costo respecto a w, y db es la derivada de la\n", "           funcion de costo respecto a b\n", "    \"\"\"\n", "    M, n = x.shape\n", "    \n", "    error = y - (x @ w + b)\n", "    \n", "    # --aqui hay que poner c\u00f3digo--        \n", "    dw = x.T @ error / M\n", "    db = error.mean()\n", "    if tipo is 'L2':\n", "        dw += (lmda/M)*(2*w)\n", "        db += (lmda/M)\n", "    elif tipo is 'L1':\n", "        dw += (lmda/M)*(np.sign(w))    \n", "    #------------------------------\n", "    return dw, db \n", "    \n", "def des_grad_regu(x, y, w_0, b_0, alpha, num_iter, lmda, tipo):\n", "    \"\"\"\n", "    Descenso de gradiente durante num_iter iteraciones para regresi\u00f3n lineal\n", "    \n", "    Par\u00e1metros\n", "    ----------\n", "    x: ndarray de dimension [M, n] con los datos de entrada\n", "    y: ndarray de dimension [M,] con los datos de salida\n", "    w_0: ndarray de dimension [n, ] con los pesos iniciales\n", "    b_0: flotante con el sesgo inicial\n", "    alpha: flotante con tama\u00f1o de paso o tasa de aprendizaje.\n", "    num_iter: numero de iteraciones (entero)\n", "    lmda: Un flotante con el valor de lambda para la regularizaci\u00f3n\n", "    tipo: string con el tipo de regularizaci\u00f3n ('L2' o 'L1')\n", "    \n", "    Devuelve\n", "    --------\n", "    w, b, mse_iter: donde w y b tiene las dimensiones de w_0 y b_0 con los par\u00e1metros aprendidos, \n", "                    mientras que mse_hist es un ndarray de dimensi\u00f3n [num_iter, 1] con el costo en cada iteraci\u00f3n.\n", "    \n", "    \"\"\"\n", "    w, b = w_0.copy(), b_0\n", "    costo_iter = np.zeros(num_iter)    \n", "    M, n = x.shape\n", "    \n", "    for iter in range(num_iter):\n", "        dw, db = grad_costo(x, y, w, b)\n", "            \n", "        # --aqui hay que poner c\u00f3digo--        \n", "        w += alpha * dw\n", "        b += alpha * db\n", "        #------------------------------\n", "        \n", "        costo_iter[iter] = costo(x, y, w, b, lmda, tipo)\n", "    return w, b, costo_iter"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Y para asegurarnos que funciona vamos a probar con un problema de juguete<br>\n", "que se puede resolver  mano para estar seguros que est\u00e1 bien programado"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w, b = np.array([1., 2., -2., 1., 1.]), 0.1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["assert reg(w, 'L1') == 7\n", "assert reg(w, 'L2') == 11"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = np.array([[1, 0, 0,  0, -1],\n", "              [0, 1, -1, 0,  0]])\n", "y = np.array([0.5, 3.8])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for lmbda in [0, .1, 1, 10, 100]:\n", "    assert abs(costo(x, y, w, b, lmbda, tipo='L1') - (0.25 / 4  + lmbda * 7 / 2)) < 1e-6\n", "    assert abs(costo(x, y, w, b, lmbda, tipo='L2') - (0.25 / 4  + lmbda * 11 / 2)) < 1e-6\n", "    dw, db = grad_costo_reg(x, y, w, b, lmbda, tipo='L2')\n", "    assert np.abs(dw - (np.array([-.2, .15, -.15, 0, .2]) + lmbda * w)).sum() < 1e-6\n", "    dw, db = grad_costo_reg(x, y, w, b, lmbda, tipo='L1')\n", "    assert np.abs(dw - (np.array([-.2, .15, -.15, 0, .2]) + lmbda * np.sign(w) / 2)).sum() < 1e-6"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora, yo voy a probr con la regularizaci\u00f3n, pero utilizando el m\u00e9todo anal\u00edtico por pseudoinversa que es muy eficiente, y les dejo a ustedes la tarea de batallar con el descenso de gradiente, con el fin que vean la ventaja de utilizar regularizaci\u00f3n. Para esto, vamos a resolver el problema con la regularizaci\u00f3n $L2$, para la cual existe ua soluci\u00f3n anal\u00edtica muy parecida a la pseudoinversa..<br>\n", "<br>\n", "Todos los pasos para derivar la regularizaci\u00f3n en $L2$ y algunas sobre la regularizaci\u00f3n en $L1$ se pueden encontrar [en esta presentaci\u00f3n](http://eniac.cs.qc.cuny.edu/andrew/gcml/lecture5.pdf). La formula anal\u00edtica para la regresi\u00f3n lineal con regularizaci\u00f3n $L2$ est\u00e1 dada por:<br>\n", "<br>\n", "$$<br>\n", "w^* = (X^TX + \\lambda I_0)^{-1}X^T Y<br>\n", "$$<br>\n", "donde $I_0$ es una matriz diagonal a la cual el elemento $I_{1,1}$ se asigna a $0$ (para no incluir el sesgo en la regularizaci\u00f3n).<br>\n", "<br>\n", "Por ejemplo, si hacemos la regresion lineal con un polinomio de d\u00e9cimo grado, conforme $\\lambda$ aumenta, el error de validaci\u00f3n y de prueba se modifican de la siguiente manera:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["grado = 10\n", "x_g = np.power(x_train.reshape(-1,1), range(0, grado + 1))\n", "x_g_plot = np.power(x_plot.reshape(-1,1), range(0, grado + 1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["M_train = x_train.shape[0]\n", "M_plot = x_plot.shape[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["I_0 = np.eye(grado + 1)\n", "I_0[0, 0] = 0.0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lambdas = 0.001 * np.array([0, .01, .05, 0.1, 0.5, 1, 5, 10, 50, 100, 500])\n", "e_train = np.zeros_like(lambdas)\n", "e_verif = np.zeros_like(lambdas)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for (i, lambd) in enumerate(lambdas):\n", "    w = np.linalg.inv(x_g.T @ x_g + (lambd * I_0)) @ x_g.T @ y_train\n", "    error_train = y_train - x_g @ w\n", "    error_verif = y_plot - x_g_plot @ w\n", "    \n", "    e_train[i] = (error_train.T @ error_train) / (2 * M_train)\n", "    e_verif[i] = (error_verif.T @ error_verif) / (2 * M_plot)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.loglog(lambdas, e_train, label=\"Error en muestra\")\n", "plt.loglog(lambdas, e_verif, label=\"Estimado de error fuera de muestra\")\n", "plt.legend()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Prueba para distintos polinomios y responde la pregunta \u00bfPorqu\u00e9 se comporta de la manera que lo hace con el polinomio de orden 10?**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora vamos a ver que pasa con las diferentes regularizaci\u00f3ne con el polinomio si vemos la predicci\u00f3n que realiza"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["grado = 10"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_g = np.power(x_train.reshape(-1,1), range(0, grado + 1))\n", "x_g_plot = np.power(x_plot.reshape(-1,1), range(0, grado + 1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(x_plot, y_plot, label=\"La funci\u00f3n original\", linewidth=5)\n", "plt.scatter(x_train, y_train, label=\"datos para el entrenamiento\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["I_0 = np.eye(grado + 1)\n", "I_0[0, 0] = 0.0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lambdas = 0.001 * np.array([0, .01, .05, 0.1, 0.5, 1, 5, 10, 50, 100, 500])\n", "for lambd in lambdas:\n", "    w = np.linalg.inv(x_g.T @ x_g + (lambd * I_0)) @ x_g.T @ y_train\n", "    y_est = x_g_plot @ w\n", "    plt.plot(x_plot, y_est, label=\"polinomio orden {} y lambda = {}\".format(grado, lambd))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.legend()\n", "plt.axis([0, 10, -0.5, 1])   "]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Escribe ahora tu funci\u00f3n de regularizaci\u00f3n utilizando el descenso de gradiente**<br>\n", "<br>\n", "Espacio para escribir conclusiones"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": [" --- Agrega aqui tu c\u00f3digo ----"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(x_plot, y_plot, label=\"La funci\u00f3n original\")\n", "plt.scatter(x_train, y_train, label=\"datos para el entrenamiento\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lambdas = 0.001 * np.array([0, .01, .05, 0.1, 0.5, 1, 5, 10, 50, 100, 500])\n", "for lambd in lambdas:\n", "    g=8\n", "    w_ini = np.random.randn(g)\n", "    # Expansi\u00f3n a un polinomio de orden ng\n", "    x_g = np.power(x_train.reshape(-1,1), range(1, g + 1))\n", "    x_g_plot = np.power(x_plot.reshape(-1,1), range(1, g + 1))\n\n", "    # Escribe aqui el c\u00f3digo  \n", "    #  1. Normaliza (ambos conjuntos)\n", "    m,d = obtiene_medias_desviaciones(x_g)\n", "    x_g = normaliza(x_g,m,d)\n", "    x_g_plot = normaliza(x_g_plot,m,d)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    #  2. Entrenamiento (con los datos de aprendizaje)\n", "    w,b,mse = des_grad_regu(x_g, y_train, w_ini, 0.0, 0.002, 50000, lambd, \"L2\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    #  3. Reconocimiento (con lo datos de graficaci\u00f3n). El resultado se guarda en y_g\n", "    y_g = x_g_plot @ w + b"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    plt.plot(x_plot, y_g, label=\"polinomio orden {}\".format(g))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.axis([0, 10, -0.5, 1]) \n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}