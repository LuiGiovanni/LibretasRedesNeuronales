{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "# coding: utf-8"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"imagenes/rn3.png\" width=\"200\"><br>\n", "<img src=\"http://www.identidadbuho.uson.mx/assets/letragrama-rgb-150.jpg\" width=\"200\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# [Curso de Redes Neuronales](https://curso-redes-neuronales-unison.github.io/Temario/)<br>\n", "<br>\n", "# Una sola neurona log\u00edstica<br>\n", "<br>\n", "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/), 17 de septiembre de 2018.<br>\n", "<br>\n", "En esta libreta vamos a revisar los aspectos b\u00e1sicos del aprendizaje para una sola neurona de salida log\u00edstica, sin capas ocultas y usando el criterio de p\u00e9rdida de entropia en dos clases. El algoritmo es sencillo pero es importante entenderlo bien antes de pasar a cosas m\u00e1s complicadas.<br>\n", "<br>\n", "Empecemos por inicializar los modulos que vamos a requerir."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["get_ipython().run_line_magic('matplotlib', 'inline')\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from IPython.display import Image  # Esto es para desplegar im\u00e1genes en la libreta"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.rcParams['figure.figsize'] = (20,10)\n", "plt.style.use('ggplot')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Funci\u00f3n log\u00edstica, funci\u00f3n de p\u00e9rdida y gradiente de la funci\u00f3n de costo"]}, {"cell_type": "markdown", "metadata": {}, "source": ["La funci\u00f3n log\u00edstica est\u00e1 dada por <br>\n", "<br>\n", "$$<br>\n", "g(z) = \\frac{1}{1 + e^{-z}},<br>\n", "$$<br>\n", "<br>\n", "la cual es importante que podamos calcular en forma vectorial. Si bien el calculo es de una sola linea, el uso de estas funciones auxiliares facilitan la legibilidad del c\u00f3digo.<br>\n", "<br>\n", "#### Desarrolla la funci\u00f3n log\u00edstica, la cual se calcule para todos los elementos de un ndarray."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def logistica(z):\n", "    \"\"\"\n", "    Calcula la funci\u00f3n log\u00edstica para cada elemento de z\n", "    \n", "    @param z: un ndarray\n", "    @return: un ndarray de las mismas dimensiones que z\n", "    \"\"\"\n", "    # Introduce c\u00f3digo aqui (una linea de c\u00f3digo)\n", "    #---------------------------------------------------\n", "    return 1/(1 + np.exp(-z))\n", "    #---------------------------------------------------\n", "    \n", "# prueba que efectivamente funciona la funci\u00f3n implementada\n", "# si el assert es falso regresa un error de aserci\u00f3n (el testunit de los pobres)\n", "assert (np.abs(logistica(np.array([-1, 0, 1])) - np.array([ 0.26894142, 0.5, 0.73105858]))).sum() < 1e-6"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Para probar la funci\u00f3n vamos a graficar la funci\u00f3n log\u00edstica en el intervalo [-5, 5]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["z = np.linspace(-5, 5, 100)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot( z, logistica(z))\n", "plt.title(u'Funci\u00f3n log\u00edstica', fontsize=20)\n", "plt.xlabel(r'$z$', fontsize=20)\n", "plt.ylabel(r'$\\frac{1}{1 + \\exp(-z)}$', fontsize=26)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Una vez establecida la funci\u00f3n log\u00edstica, vamos a implementar la funci\u00f3n de p\u00e9rdida *sin regularizar* que se utiliza t\u00edpicamente en clasificaci\u00f3n binaria, la cual est\u00e1 dada por<br>\n", "<br>\n", "$$<br>\n", "Loss(w, b) = -\\frac{1}{M} \\sum_{i=1}^M \\left[ y^{(i)}\\log(a^{(i)}) + (1 - y^{(i)})\\log(1 - a^{(i)})\\right],<br>\n", "$$<br>\n", "<br>\n", "donde <br>\n", "<br>\n", "$$<br>\n", "a^{(i)} = g(z^{(i)}), \\quad\\quad z^{(i)} = w^T x^{(i)} + b<br>\n", "$$<br>\n", "<br>\n", "las cuales fueron ecuaciones revisadas en clase.<br>\n", "<br>\n", "#### Implementa la funci\u00f3n de p\u00e9rdida para un conjunto de aprendizaje."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def Loss(x, y, w, b):\n", "    \"\"\"\n", "    Calcula el costo de una w dada para el conjunto dee entrenamiento dado por y y x\n", "    \n", "    @param x: un ndarray de dimensi\u00f3n (M, n) con la matriz de dise\u00f1o\n", "    @param y: un ndarray de dimensi\u00f3n (M, ) donde cada entrada es 1.0 o 0.0\n", "    @param w: un ndarray de dimensi\u00f3n (n, ) con los pesos\n", "    @param b: un flotante con el sesgo\n", "    @return: un flotante con el valor de p\u00e9rdida\n", "    \n", "    \"\"\" \n", "    M = x.shape[0]\n\n", "    #------------------------------------------------------------------------\n", "    sigma = logistica(x @ w + b)\n", "    return (1/M)*(np.sum(-y*np.log(sigma)-(1-y)*np.log(1-sigma)))\n", "    \n", "    #------------------------------------------------------------------------\n", "    \n", "# Otra vez el testunit del pobre (ya lo calcule yo, pero puedes hacerlo a mano para estar seguro)\n", "w = np.array([1])\n", "b = 1.0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = np.array([[10],\n", "              [-5]])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y1 = np.array([1, 0])\n", "y2 = np.array([0, 1])\n", "y3 = np.array([0, 0])\n", "y4 = np.array([1, 1])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["assert abs(Loss(x, y1, w, b) - 0.01) < 1e-2\n", "assert abs(Loss(x, y2, w, b) - 7.5) < 1e-2\n", "assert abs(Loss(x, y3, w, b) - 5.5) < 1e-2\n", "assert abs(Loss(x, y4, w, b) - 2.0) < 1e-2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["De la misma manera, para poder implementar las funciones de aprendizaje, vamos a implementar el gradiente de la funci\u00f3n de p\u00e9rdida. El gradiente de la funci\u00f3n de p\u00e9rdida respecto a $\\omega$ es (como lo vimos en clase) el siguiente:<br>\n", "<br>\n", "$$<br>\n", "\\frac{\\partial Loss(w, b)}{\\partial w_j} = -\\frac{1}{M} \\sum_{i=1}^M \\left(y^{(i)} - a^{(i)}\\right)x_j^{(i)} <br>\n", "$$<br>\n", "<br>\n", "$$<br>\n", "\\frac{\\partial Loss(w, b)}{\\partial b} = -\\frac{1}{M} \\sum_{i=1}^M \\left(y^{(i)} - a^{(i)}\\right) <br>\n", "$$<br>\n", "<br>\n", "y a partir de las ecuaciones individuales de puede obtener $\\nabla Loss(\\omega)$, la cual no la vamos a escribir en la libreta para que revisen en sus notas como se puede resolver este problema en forma matricial. <br>\n", "<br>\n", "#### IImplementa (con operaciones matriciales) el calculo del gradiente de la funci\u00f3n de p\u00e9rdida."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def gradiente(x, y, w, b):\n", "    \"\"\"\n", "    Calcula el gradiente de la funci\u00f3n de p\u00e9rdida para clasificaci\u00f3n binaria, \n", "    utilizando una neurona log\u00edstica, para w y b y conociendo un conjunto de aprendizaje.\n", "    \n", "    @param x: un ndarray de dimensi\u00f3n (M, n) con la matriz de dise\u00f1o\n", "    @param y: un ndarray de dimensi\u00f3n (M, ) donde cada entrada es 1.0 o 0.0\n", "    @param w: un ndarray de dimensi\u00f3n (n, ) con los pesos\n", "    @param b: un flotante con el sesgo\n", "    \n", "    @return: dw, db, un ndarray de mismas dimensiones que w y un flotnte con el c\u00e1lculo de \n", "             la dervada evluada en el punto w y b\n", "                 \n", "    \"\"\"\n", "    M = x.shape[0]\n\n", "    #------------------------------------------------------------------------\n", "    # Agregua aqui tu c\u00f3digo\n", "    a = logistica(x @ w + b)\n", "    e = y-a \n", "    return -x.T @ e / M, -e.mean()    \n", "    #------------------------------------------------------------------------\n", "    \n", "    return dw, db"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    \n", "# Otra vez el testunit del pobre (ya lo calcule yo, pero puedes hacerlo a mano para estar seguro)\n", "w = np.array([1])\n", "b = 1.0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = np.array([[10],\n", "              [-5]])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y1 = np.array([1, 0])\n", "y2 = np.array([0, 1])\n", "y3 = np.array([0, 0])\n", "y4 = np.array([1, 1])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["assert abs(0.00898475 - gradiente(x, y1, w, b)[1]) < 1e-4\n", "assert abs(7.45495097 - gradiente(x, y2, w, b)[0]) < 1e-4 \n", "assert abs(4.95495097 - gradiente(x, y3, w, b)[0]) < 1e-4 \n", "assert abs(-0.49101525 - gradiente(x, y4, w, b)[1]) < 1e-4     "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Descenso de gradiente"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora vamos a desarrollar las funciones necesarias para realizar el entrenamiento y encontrar la mejor $\\omega$ de acuero a la funci\u00f3n de costos y un conjunto de datos de aprendizaje.<br>\n", "<br>\n", "Para este problema, vamos a utilizar una base de datos sint\u00e9tica proveniente del curso de [Andrew Ng](www.andrewng.org/) que se encuentra en [Coursera](https://www.coursera.org). Supongamos que pertenecemos al departamente de servicios escolares de la UNISON y vamos a modificar el procedimiento de admisi\u00f3n. En lugar de utilizar un solo ex\u00e1men (EXCOBA) y la informaci\u00f3n del cardex de la preparatoria, hemos decidido aplicar dos ex\u00e1menes, uno sicom\u00e9trico y otro de habilidades estudiantiles. Dichos ex\u00e1menes se han aplicado el \u00faltimo a\u00f1o aunque no fueron utilizados como criterio. As\u00ed, tenemos un historial entre estudiantes aceptados y resultados de los dos ex\u00e1menes. El objetivo es hacer un m\u00e9todo de regresi\u00f3n que nos permita hacer la admisi\u00f3n a la UNISON tomando en cuenta \u00fanicamente los dos ex\u00e1menes y simplificar el proceso. *Recuerda que esto no es verdad, es solo un ejercicio*.<br>\n", "<br>\n", "Bien, los datos se encuentran en el archivo `admision.txt` el cual se encuentra en formato `cvs` (osea los valores de las columnas separados por comas. Vamos a leer los datos y graficar la informaci\u00f3n para entender un poco los datos."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["datos = np.loadtxt('datos/admision.txt', comments='%', delimiter=',')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x, y = datos[:,0:-1], datos[:,-1] "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(x[y == 1, 0], x[y == 1, 1], 'sr', label='aceptados') \n", "plt.plot(x[y == 0, 0], x[y == 0, 1], 'ob', label='rechazados')\n", "plt.title(u'Ejemplo sint\u00e9tico para regresi\u00f3n log\u00edstica')\n", "plt.xlabel(u'Calificaci\u00f3n del primer examen')\n", "plt.ylabel(u'Calificaci\u00f3n del segundo examen')\n", "plt.axis([20, 100, 20, 100])\n", "plt.legend(loc=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Vistos los datos un clasificador lineal podr\u00eda ser una buena soluci\u00f3n. Ahora vamos a implementar el m\u00e9todo de descenso de gradiente, casi de la misma manera que lo implementamos para regresi\u00f3n lineal (por lotes)<br>\n", "<br>\n", "#### Implementa el descenso de gradiente para el problema de regresi\u00f3n log\u00edstica en modo batch."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def descenso_rl_lotes(x, y, alpha, max_iter=int(1e4), tol=1e-4, historial=False):\n", "    \"\"\"\n", "    Descenso de gradiente por lotes para resolver el problema de regresi\u00f3n log\u00edstica con un conjunto de aprendizaje\n", "    @param x: un ndarray de dimensi\u00f3n (M, n) con la matriz de dise\u00f1o\n", "    @param y: un ndarray de dimensi\u00f3n (M, ) donde cada entrada es 1.0 o 0.0\n", "    @param alpha: Un flotante (t\u00edpicamente peque\u00f1o) con la tasa de aprendizaje\n", "    @param tol: Un flotante peque\u00f1o como criterio de paro. Por default 1e-4\n", "    @param max_iter: M\u00e1ximo numero de iteraciones. Por default 1e4\n", "    @param historial: Un booleano para saber si guardamos el historial de la funci\u00f3n de p\u00e9rdida o no\n", "    \n", "    @return: w, b, perdida_hist donde w es ndarray de dimensi\u00f3n (n, ) con los pesos; el flotante b  \n", "             con el sesgo y perdida_hist, un ndarray de dimensi\u00f3n (max_iter,) con el valor de la funci\u00f3n \n", "             de p\u00e9rdida en cada iteraci\u00f3n. Si historial == True, entonces perdida_hist = None.\n", "             \n", "    \"\"\"\n", "    M, n = x.shape\n", "    \n", "    w = np.zeros(n)\n", "    b = 0.0\n", "    if historial:\n", "        perdida_hist = np.zeros(max_iter)\n", "        perdida_hist[0] = Loss(x, y, w, b)\n", "    else:\n", "        perdida_hist = None\n", "            \n", "    \n", "    for epoch in range(1, max_iter):\n", "        #--------------------------------------------------------------\n", "        # Agregar aqui tu c\u00f3digo\n", "        #\n", "        # Recuerda utilizar las funciones que ya has desarrollado\n", "        J = Loss ( x, y, w, b)\n", "        dw,db = gradiente(x,y,w,b)\n", "        w+= -dw*alpha\n", "        b+= -db*alpha\n", "        if J < tol:\n", "            return w, b, perdida_hist\n", "        if historial:\n", "            perdida_hist[epoch] = J\n", "        \n", "        #--------------------------------------------------------------\n", "    return w, b, perdida_hist"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Para probar la funci\u00f3n de aprendizaje, vamos a aplicarla a nuestro problema de admisi\u00f3n. Primero recuerda que tienes que hacer una exploraci\u00f3n para encontrar el mejor valor de $\\epsilon$. As\u00ed que utiliza el c\u00f3digo de abajo para ajustar $\\alpha$."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["alpha = 1e-6\n", "mi = 50\n", "_, _, perdida_hist = descenso_rl_lotes(x, y, alpha, max_iter=mi, tol=1e-4, historial=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(np.arange(mi), perdida_hist)\n", "plt.title(r'Evolucion del valor de la funci\u00f3n de p\u00e9rdida en las primeras iteraciones con $\\alpha$ = ' + str(alpha))\n", "plt.xlabel('iteraciones')\n", "plt.ylabel('perdida')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Una vez encontrado el mejor $\\epsilon$, entonces podemos calcular $\\omega$ (esto va a tardar bastante), recuerda que el costo final debe de ser lo m\u00e1s cercano a 0 posible, as\u00ed que agrega cuantas iteraciones sean necesarias (a partir de una funci\u00f3n de p\u00e9rdida con un valor de al rededor de 0.22 ya est\u00e1 bien): "]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w, b, _ = descenso_rl_lotes(x, y, alpha, max_iter = 1000000)\n", "print(\"Los pesos obtenidos son: \\n{}\".format(w))\n", "print(\"El sesgo obtenidos es: \\n{}\".format(b))\n", "print(\"El valor final de la funci\u00f3n de p\u00e9rdida es: {}\".format(Loss(x, y, w, b))) "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Es interesante ver como el descenso de gradiente no es muy eficiente en este tipo de problemas, a pesar de ser problemas de optimizaci\u00f3n convexos.<br>\n", "<br>\n", "Bueno, este m\u00e9todo nos devuelve $\\omega$, pero esto no es suficiente para decir que tenemos un clasificador, ya que un m\u00e9todo de clasificaci\u00f3n se compone de dos m\u00e9todos, uno para **aprender** y otro para **predecir**. <br>\n", "<br>\n", "Recuerda que $a^{(i)} = \\Pr[y^{(i)} = 1 | x^{(i)} ; w, b]$, y a partir de esta probabilidad debemos tomar una desici\u00f3n. Igualmente recuerda que para tomar la desicion no necesitamos calcular el valor de la log\u00edstica, si conocemos el umbral."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Desarrolla una funci\u00f3n de predicci\u00f3n."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def predictor(x, w, b):\n", "    \"\"\"\n", "    Predice los valores de y_hat (que solo pueden ser 0 o 1), utilizando el criterio MAP.\n", "    \n", "    @param x: un ndarray de dimensi\u00f3n (M, n) con la matriz de dise\u00f1o\n", "    @param w: un ndarray de dimensi\u00f3n (n, ) con los pesos\n", "    @param b: un flotante con el sesgo\n", "    @return: un ndarray de dimensi\u00f3n (M, ) donde cada entrada es 1.0 o 0.0 con la salida estimada\n", "    \"\"\"\n", "    #-------------------------------------------------------------------------------------\n", "    # Agrega aqui tu c\u00f3digo sin utilizar la funci\u00f3n log\u00edstica\n", "    salida_estimada = x @ w + b \n", "    return salida_estimada >= 0.5    \n", "    #--------------------------------------------------------------------------------------\n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u00bfQue tan bueno es este clasificador? \u00bfEs que implementamos bien el m\u00e9todo?<br>\n", "<br>\n", "Vamos a contestar esto por partes. Primero, vamos a graficar los mismos datos pero vamos a agregar la superficie de separaci\u00f3n, la cual en este caso sabemos que es una linea recta. Como sabemos el criterio para decidir si un punto pertenece a la clase distinguida o no es si el valor de $w^T x^{(i)} + b \\ge 0$, por lo que la frontera entre la regi\u00f3n donde se escoge una clase de otra se encuentra en:<br>\n", "<br>\n", "$$<br>\n", "0 = b + w_1 x_1  + w_2 x_2,<br>\n", "$$<br>\n", "<br>\n", "y despejando:<br>\n", "<br>\n", "$$<br>\n", "x_2 = -\\frac{b}{w_2} -\\frac{w_1}{w_2}x_1<br>\n", "$$<br>\n", "<br>\n", "son los pares $(x_1, x_2)$ los valores en la forntera. Al ser estos (en este caso) una linea recta solo necesitamos dos para graficar la superficie de separaci\u00f3n. <br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x1_frontera = np.array([20, 100]) #Los valores m\u00ednimo y m\u00e1ximo que tenemos en la gr\u00e1fica de puntos\n", "x2_frontera = -(b / w[1]) - (w[0] / w[1]) * x1_frontera"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(x[y == 1, 0], x[y == 1, 1], 'sr', label='aceptados') \n", "plt.plot(x[y == 0, 0], x[y == 0, 1], 'ob', label='rechazados')\n", "plt.plot(x1_frontera, x2_frontera, 'm')\n", "plt.title(u'Ejemplo sint\u00e9tico para regresi\u00f3n log\u00edstica')\n", "plt.xlabel(u'Calificaci\u00f3n del primer examen')\n", "plt.ylabel(u'Calificaci\u00f3n del segundo examen')\n", "plt.axis([20, 100, 20, 100])\n", "plt.legend(loc=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Y para que tengas una idea de lo que deber\u00eda de salir, anexo una figura obtenida con el c\u00f3digo que yo hice:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Image(filename='imagenes/ejemplo_logistica.png')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Clasificaci\u00f3n polinomial<br>\n", "<br>\n", "Como podemos ver en la gr\u00e1fica de arriba, parece ser que la regresi\u00f3n log\u00edstica aceptar\u00eda a algunos estudiantes rechazados y rechazar\u00eda a algunos que si fueron en realidad aceptados. En todo m\u00e9todo de clasificaci\u00f3n hay un grado de error, y eso es parte del poder de generalizaci\u00f3n de los m\u00e9todos. <br>\n", "<br>\n", "Sin embargo, una simple inspecci\u00f3n visual muestra que, posiblemente, la regresi\u00f3n lineal no es la mejor soluci\u00f3n, ya que la frontera entre las dos clases parece ser m\u00e1s bien una curva.<br>\n", "<br>\n", "\u00bfQue tal si probamos con un clasificador cuadr\u00e1tico? Un clasificador cuadr\u00e1tico no es m\u00e1s que la regresi\u00f3n lineal pero a la que se le agregan todos los atributos que sean una combinaci\u00f3n de dos de los atributos. <br>\n", "<br>\n", "Por ejemplo, si un ejemplo $x = (x_1, x_2, x_3)^T$ se aumenta con todas sus componentes cuadr\u00e1ticas, entonces tenemos los atributos<br>\n", "<br>\n", "$$<br>\n", "\\phi_2(x) = (x_1, x_2, x_3, x_1 x_2, x_1 x_3, x_2 x_3, x_1^2, x_2^2, x_3^2)^T.<br>\n", "$$ <br>\n", "<br>\n", "De la misma manera se pueden obtener clasificadores de orden tres, cuatro, cinco, etc. En general a estos clasificadores se les conoce como **clasificadores polinomiales**. Ahora, para entender bien la idea, vamos a resolver el problema anterior con un clasificador de orden 2. <br>\n", "<br>\n", "Sin embargo, si luego se quiere hacer el reconocimiento de otros objetos, o cambiar el orden del polinomio, pues se requerir\u00eda de reclcular cada vez la expansi\u00f3n polinomial. Vamos a generalizar la obtenci\u00f3n de atributos polinomiales con la funci\u00f3n `map_poly`, la cual la vamos a desarrollar a continuaci\u00f3n.<br>\n", "<br>\n", "En este caso, la normalizaci\u00f3n de los datos es muy importante, por lo que se agregan las funciones pertinentes.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from itertools import combinations_with_replacement"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def map_poly(grad, x):\n", "    \"\"\"\n", "    Encuentra las caracter\u00edsticas polinomiales hasta el grado grad de la matriz de datos x, \n", "    asumiendo que x[:n, 0] es la expansi\u00f3n de orden 1 (los valores de cada atributo)\n", "    \n", "    @param grad: un entero positivo con el grado de expansi\u00f3n\n", "    @param x: un ndarray de dimension (M, n) donde n es el n\u00famero de atributos\n", "    \n", "    @return: un ndarray de dimensi\u00f3n (M, n_phi) donde\n", "             n_phi = \\sum_{i = 1}^grad fact(i + n - 1)/(fact(i) * fact(n - 1))\n", "    \"\"\"\n", "    \n", "    if int(grad) < 2:\n", "        raise ValueError('grad debe de ser mayor a 1')\n", "    \n", "    M, n = x.shape\n", "    atrib = x.copy()\n", "    x_phi = x.copy()\n", "    for i in range(2, int(grad) + 1):\n", "        for comb in combinations_with_replacement(range(n), i):\n", "            x_phi = np.c_[x_phi, np.prod(atrib[:, comb], axis=1)]\n", "    return x_phi   "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def medias_std(x):\n", "    \"\"\"\n", "    Obtiene un vector de medias y desviaciones estandar para normalizar\n", "    \n", "    @param x: Un ndarray de (M, n) con una matriz de dise\u00f1o\n", "    \n", "    @return: mu, des_std dos ndarray de dimensiones (n, ) con las medias y desviaciones estandar\n", "    \n", "    \"\"\"\n", "    return np.mean(x, axis=0), np.std(x, axis=0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def normaliza(x, mu, des_std):\n", "    \"\"\"\n", "    Normaliza los datos x\n", "    \n", "    @param x: un ndarray de dimension (M, n) con la matriz de dise\u00f1o\n", "    @param mu: un ndarray (n, ) con las medias\n", "    @param des_std: un ndarray (n, ) con las desviaciones estandard\n", "    \n", "    @return: un ndarray (M, n) con x normalizado\n", "    \n", "    \"\"\"\n", "    return (x - mu) / des_std"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Realiza la clasificaci\u00f3n de los datos utilizando un clasificador cuadr\u00e1tico (recuerda ajustar primero el valor de $\\alpha$)**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Encuentra phi_x (x son la expansi\u00f3n polinomial de segundo orden, utiliza la funci\u00f3n map_poly"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["phi_x =  map_poly(2, x) #--Agrega el c\u00f3digo aqui--\n", "mu, de = medias_std(phi_x)\n", "phi_x_norm = normaliza(phi_x, mu, de)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Utiliza la regresi\u00f3n log\u00edstica"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["alpha = 1 #--Agrega el dato aqui--\n", "max_iter =  50#--Agrega el dato aqui--\n", "_, _, hist_loss = descenso_rl_lotes(phi_x_norm, y, alpha, max_iter, historial=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(range(len(hist_loss)), hist_loss)\n", "plt.xlabel('epochs')\n", "plt.ylabel('Loss')\n", "plt.title('Evaluaci\u00f3n del par\u00e1metro alpha')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w_norm, b_norm, _ = descenso_rl_lotes(phi_x_norm, y, alpha, 1000)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Los pesos obtenidos son: \\n{}\".format(w_norm))\n", "print(\"El sesgo obtenidos es: \\n{}\".format(b_norm))\n", "print(\"El valor final de la funci\u00f3n de p\u00e9rdida es: {}\".format(Loss(phi_x_norm, y, w_norm, b_norm))) "]}, {"cell_type": "markdown", "metadata": {}, "source": ["donde se puede encontrar un valor de p\u00e9rdida de aproximadamente 0.03.<br>\n", "<br>\n", "Esto lo tenemos que graficar. Pero graficar la separaci\u00f3n de datos en una proyecci\u00f3n en las primeras dos dimensiones, no es tan sencillo como lo hicimos con una separaci\u00f3n lineal, as\u00ed que vamos atener que generar un `contour`, y sobre este graficar los datos. Para esto vamos a desarrollar una funci\u00f3n."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_separacion2D(x, y, grado, mu, de, w, b):\n", "    \"\"\"\n", "    Grafica las primeras dos dimensiones (posiciones 1 y 2) de datos en dos dimensiones \n", "    extendidos con un clasificador polinomial as\u00ed como la separaci\u00f3n dada por theta_phi\n", "    \n", "    \"\"\"\n", "    if grado < 2:\n", "        raise ValueError('Esta funcion es para graficar separaciones con polinomios mayores a 1')\n", "    \n", "    x1_min, x1_max = np.min(x[:,0]), np.max(x[:,0])\n", "    x2_min, x2_max = np.min(x[:,1]), np.max(x[:,1])\n", "    delta1, delta2 = (x1_max - x1_min) * 0.1, (x2_max - x2_min) * 0.1\n", "    spanX1 = np.linspace(x1_min - delta1, x1_max + delta1, 600)\n", "    spanX2 = np.linspace(x2_min - delta2, x2_max + delta2, 600)\n", "    X1, X2 = np.meshgrid(spanX1, spanX2)\n", "    X = normaliza(map_poly(grado, np.c_[X1.ravel(), X2.ravel()]), mu, de)\n", "        \n", "    Z = predictor(X, w, b)\n", "    Z = Z.reshape(X1.shape[0], X1.shape[1])\n", "    \n", "    # plt.contour(X1, X2, Z, linewidths=0.2, colors='k')\n", "    plt.contourf(X1, X2, Z, 1, cmap=plt.cm.binary_r)\n", "    plt.plot(x[y > 0.5, 0], x[y > 0.5, 1], 'sr', label='clase positiva')\n", "    plt.plot(x[y < 0.5, 0], x[y < 0.5, 1], 'oy', label='clase negativa')\n", "    plt.axis([spanX1[0], spanX1[-1], spanX2[0], spanX2[-1]])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Y ahora vamos a probar la funci\u00f3n `plot_separacion2D` con los datos de entrenamiento. El comando tarda, ya que estamos haciendo un grid de 200 $\\times$ 200, y realizando evaluaciones individuales."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_separacion2D(x, y, 2, mu, de, w_norm, b_norm)\n", "plt.title(u\"Separaci\u00f3n con un clasificador cuadr\u00e1tico\")\n", "plt.xlabel(u\"Calificaci\u00f3n del primer examen\")\n", "plt.ylabel(u\"Calificaci\u00f3n del segundo examen\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Y para dar una idea de lo que deber\u00eda de salir, anexo una figura tal como me sali\u00f3 a mi"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Image(filename='imagenes/ejemplo_cuad.png')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Como podemos ver, un clasificador polinomial de orden 2 clasifica mejor los datos de aprendizaje, y adem\u00e1s parece suficientemente simple para ser la mejor opci\u00f3n para hacer la predicci\u00f3n. Claro, esto lo sabemos porque pudimos visualizar los datos, y en el fondo estamos haciendo trampa, al seleccionar la expansi\u00f3n polinomial a partir de una inspecci\u00f3n visual de los datos.<br>\n", "<br>\n", "Tomemos ahora una base de datos que si bien es sint\u00e9tica es representativa de una familia de problemas a resolver. Supongamos que est\u00e1mos opimizando la fase de pruebas dentro de la linea de producci\u00f3n de la empresa Microprocesadores del Noroeste S.A. de C.V.. La idea es reducir el banco de pruebas de cada nuevo microprocesador fabricado y en lugar de hacer 50 pruebas, reducirlas a 2. En el conjunto de datos tenemos los valores que obtuvo cada componente en las dos pruebas seleccionadas, y la decisi\u00f3n que se tomo con cada dispositivo (esta desici\u00f3n se tomo con el banco de 50 reglas). Los datos los podemos visualizar a continuaci\u00f3n."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["datos = np.loadtxt('datos/prod_test.txt', comments='%', delimiter=',')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x, y = datos[:,0:-1], datos[:,-1] "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(x[y == 1, 0], x[y == 1, 1], 'or', label='cumple calidad') \n", "plt.plot(x[y == 0, 0], x[y == 0, 1], 'ob', label='rechazado')\n", "plt.title(u'Ejemplo de pruebas de un producto')\n", "plt.xlabel(u'Valor obtenido en prueba 1')\n", "plt.ylabel(u'Valor obtenido en prueba 2')\n", "#plt.axis([20, 100, 20, 100])\n", "plt.legend(loc=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cl\u00e1ramente este problema no se puede solucionar con un clasificador lineal (1 orden), por lo que hay que probar otros tipos de clasificadores.<br>\n", "<br>\n", "**Completa el c\u00f3digo para hacer regresi\u00f3n polinomial para polinomios de orden 2, 4, 6 y 8, y muestra los resultados en una figura. Recuerda que este ejercicio puede tomar bastante tiempo de c\u00f3mputo. Posiblemente tengas que hacer ajustes en el c\u00f3digo para manejar diferentes valores de alpha y max_iter de acuerdo a cada caso**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for (i, grado) in enumerate([2, 6, 10, 14]):\n\n", "    # Genera la expansi\u00f3n polinomial\n", "    # --- Agregar c\u00f3digo aqu\u00ed ---\n", "    phi_x = map_poly(grado,x)\n", "    \n", "    # Normaliza\n", "    # --- Agregar c\u00f3digo aqu\u00ed ---\n", "    mean, std = medias_std(phi_x)\n", "    phi_x_norm = normaliza(phi_x, mean, std)\n", "    \n", "    # Entrena\n", "    # --- Agregar c\u00f3digo aqu\u00ed ---\n", "    alpha =  0.1\n", "    max_iter = 60000\n", "    w_norm, b_norm, perdida_h = descenso_rl_lotes(phi_x_norm, y, alpha, max_iter)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # Muestra resultados con plot_separacion2D \n", "    plt.subplot(2, 2, i + 1)\n", "    plt.title(\"Polinomio de grado {}\".format(grado))\n", "    # --- Agregar codigo aqu\u00ed ---\n", "    plot_separacion2D(x, y, grado, mean, std, w_norm, b_norm)\n", "    \n", "    \n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["y un resultado parcial (no verifiqu\u00e9 el m\u00ednimo global en los cuatro casos) es el que pongo en la figura siguente:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Image(filename='imagenes/ejemplo_poly.png')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Regularizaci\u00f3n<br>\n", "<br>\n", "Como podemos ver del ejercicio anterior, es dificil determinar el grado del polinomio, y en algunos casos es demasiado general (subaprendizaje) y en otros demasiado espec\u00edfico (sobreaprendizaje). \u00bfQue podr\u00eda ser la soluci\u00f3n?, bueno, una soluci\u00f3n posible es utilizar un polinomio de alto grado (o relativamente alto), y utilizar la **regularizaci\u00f3n** para controlar la generalizaci\u00f3n del algoritmo, a trav\u00e9s de una variable $\\lambda$.<br>\n", "<br>\n", "Recordemos, la funci\u00f3n de costos de la regresi\u00f3n log\u00edstica con regularizaci\u00f3n es:<br>\n", "<br>\n", "$$<br>\n", "costo(w, b) = Loss(w, b) + \\frac{\\lambda}{M} regu(w),<br>\n", "$$<br>\n", "<br>\n", "donde $regu(w)$ es una regularizaci\u00f3n, la cual puede ser $L_1$, $L_2$ u otras, tal como vimos en clase. Como en la libreta de regresi\u00f3n lineal ya algo se hizo con la regularizaci\u00f3n, aqui nos vamos a pasar directamente a calcular el costo, el gradiente regularizado y la modificaci\u00f3n al m\u00e9todo de descenso de grdiente por lotes que tenemos programado.<br>\n", "<br>\n", "**Completa el siguiente c\u00f3digo, utilizando una regularizaci\u00f3n en $L_2$**<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def costo(x, y, w, b, lambd):\n", "    \"\"\"\n", "    Calcula el costo de una w dada para el conjunto dee entrenamiento dado por y y x,\n", "    usando regularizaci\u00f3n\n", "    \n", "    @param x: un ndarray de dimensi\u00f3n (M, n) con la matriz de dise\u00f1o\n", "    @param y: un ndarray de dimensi\u00f3n (M, ) donde cada entrada es 1.0 o 0.0\n", "    @param w: un ndarray de dimensi\u00f3n (n, ) con los pesos\n", "    @param b: un flotante con el sesgo\n", "    @param lambd: un flotante con el valor de lambda en la regularizacion\n", "    @return: un flotante con el valor de p\u00e9rdida\n", "    \n", "    \"\"\" \n", "    M = x.shape[0]\n", "    \n", "    #------------------------------------------------------------------------\n", "    # Agregua aqui tu c\u00f3digo\n", "    return Loss(x,y,w,b) + (lambd / M) * w.T @ w\n", "    \n", "    \n", "    #------------------------------------------------------------------------"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def grad_regu(x, y, w, b, lambd):\n", "    \"\"\"\n", "    Calcula el gradiente de la funci\u00f3n de costo regularizado para clasificaci\u00f3n binaria, \n", "    utilizando una neurona log\u00edstica, para w y b y conociendo un conjunto de aprendizaje.\n", "    \n", "    @param x: un ndarray de dimensi\u00f3n (M, n) con la matriz de dise\u00f1o\n", "    @param y: un ndarray de dimensi\u00f3n (M, ) donde cada entrada es 1.0 o 0.0\n", "    @param w: un ndarray de dimensi\u00f3n (n, ) con los pesos\n", "    @param b: un flotante con el sesgo\n", "    @param lambd: un flotante con el peso de la regularizaci\u00f3n\n", "    \n", "    @return: dw, db, un ndarray de mismas dimensiones que w y un flotnte con el c\u00e1lculo de \n", "             la dervada evluada en el punto w y b\n", "                 \n", "    \"\"\"\n", "    M = x.shape[0]\n\n", "    #------------------------------------------------------------------------\n", "    # Agregua aqui tu c\u00f3digo\n", "    a = logistica(x @ w + b)\n", "    e = y-a \n", "    return (-x.T @ e + lambd*w)/ M, -e.mean()\n", "    \n", "    #------------------------------------------------------------------------\n", "    \n", "    return dw, db"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def desc_lotes_regu(x, y, alpha, lambd, max_iter=int(1e4), tol=1e-4, historial=False):\n", "    \"\"\"\n", "    Descenso de gradiente por lotes para resolver el problema de regresi\u00f3n log\u00edstica con un conjunto de aprendizaje\n", "    @param x: un ndarray de dimensi\u00f3n (M, n) con la matriz de dise\u00f1o\n", "    @param y: un ndarray de dimensi\u00f3n (M, ) donde cada entrada es 1.0 o 0.0\n", "    @param alpha: Un flotante (t\u00edpicamente peque\u00f1o) con la tasa de aprendizaje\n", "    @param lambd: Un flotante con el valor de la regularizaci\u00f3n\n", "    @param max_iter: M\u00e1ximo numero de iteraciones. Por default 1e4\n", "    @param tol: Un flotante peque\u00f1o como criterio de paro. Por default 1e-4\n", "    @param historial: Un booleano para saber si guardamos el historial de la funci\u00f3n de p\u00e9rdida o no\n", "    \n", "    @return: w, b, perdida_hist donde w es ndarray de dimensi\u00f3n (n, ) con los pesos; el flotante b  \n", "             con el sesgo y perdida_hist, un ndarray de dimensi\u00f3n (max_iter,) con el valor de la funci\u00f3n \n", "             de p\u00e9rdida en cada iteraci\u00f3n. Si historial == True, entonces perdida_hist = None.\n", "             \n", "    \"\"\"\n", "    M, n = x.shape\n", "    \n", "    w = 0 * np.random.random(n) - 0.1\n", "    b = 0 * np.random.random() - 0.1\n", "    if historial:\n", "        perdida_hist = np.zeros(max_iter)\n", "        perdida_hist[0] = costo(x, y, w, b, lambd)\n", "    else:\n", "        perdida_hist = None\n", "            \n", "    \n", "    for iter in range(1, max_iter):\n", "        #--------------------------------------------------------------\n", "        # Agregar aqui tu c\u00f3digo\n", "        #\n", "        # Recuerda utilizar las funciones que ya has desarrollado\n", "        J =  costo( x, y, w, b,lambd)\n", "        dw,db = grad_regu(x,y,w,b,lambd)\n", "        w+= -dw*alpha\n", "        b+= -db*alpha\n", "        if J < tol:\n", "            return w, b, perdida_hist\n", "        if historial:\n", "            perdida_hist[iter] = J        \n", "        \n", "        #--------------------------------------------------------------\n", "    return w, b, perdida_hist"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<br>\n", "**Desarrolla las funciones y scriprs necesarios para realizar la regresi\u00f3n log\u00edstica con un polinomio de grado 14 y con cuatro valores de regularizaci\u00f3n diferentes. Grafica la superficie de separaci\u00f3n para cuatro valores diferentes de $\\lambda$. Escribe tus conclusiones** "]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["phi_x = map_poly(14, x)\n", "mu, de = medias_std(phi_x)\n", "phi_x_norm = normaliza(phi_x, mu, de)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for (i, lambd) in enumerate([0, 1, 10, 100]):\n\n", "    # Normaliza\n", "    # --- Agregar c\u00f3digo aqu\u00ed ---\n", "    \n", "    # Entrena\n", "    # --- Agregar c\u00f3digo aqu\u00ed ---\n", "    w_norm, b_norm, perdida_hist = desc_lotes_regu(phi_x_norm, y, alpha, lambd, max_iter, historial=True)\n", "    # Muestra resultados con plot_separacion2D \n", "    plt.subplot(2, 2, i + 1)\n", "    plt.title(\"Polinomio de grado 14, regu = {}.\".format(lambd))\n", "    # --- Agregar codigo aqu\u00ed ---\n", "    plot_separacion2D(x, y, grado, mu, de, w_norm, b_norm)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["y tambien pongo unos resultados preliminares para que los contrasten con los suyos."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Image(filename='imagenes/ejemplo_regu.png')"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}